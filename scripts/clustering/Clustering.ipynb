{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fddbf330-399a-4939-b345-ea2a28446cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Used to accelerate plotting DAMON figures.\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "\n",
    "from matplotlib.colors import LogNorm, hsv_to_rgb\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def apply_cluster(page_stat_df):\n",
    "    scaler = StandardScaler()\n",
    "    features = page_stat_df.drop(columns=['PageFrame', 'rno', 'duty_cycle_sample_count', 'duty_cycle'])\n",
    "    #print(features)\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    \n",
    "    #pca_col = ['pc1', 'pc2']\n",
    "    pca = PCA(n_components=0.95)\n",
    "    pca_df = pd.DataFrame(pca.fit_transform(scaled_features))#, columns=pca_col)\n",
    "    \n",
    "    k = 4\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    #kmeans.fit(scaled_features)\n",
    "    #kmeans.fit(pca_df)\n",
    "    db = DBSCAN(eps=1.0, min_samples=5).fit(pca_df)\n",
    "    \n",
    "    #page_stat_df['cluster'] = kmeans.labels_\n",
    "    page_stat_df['cluster'] = db.labels_\n",
    "    page_stat_df['cluster'] = page_stat_df['cluster'].astype(int)\n",
    "\n",
    "    #print('----page_stat_df-----')\n",
    "    #print(page_stat_df)\n",
    "    #print('----------')\n",
    "    #print('pca_df')\n",
    "    #print(pca_df)\n",
    "    #print('-----')\n",
    "    \n",
    "    page_stat_df_merged = (pd.concat([page_stat_df, pca_df], axis=1))\n",
    "    page_stat_df_merged['start_epoch'] = 0\n",
    "\n",
    "    # Return new df with cluster labels and pca values\n",
    "    return page_stat_df_merged\n",
    "\n",
    "def find_region_id(row, df2):\n",
    "    #print(row)\n",
    "    #time = row['time']\n",
    "    addr = row['PageFrame']\n",
    "    matches = df2[\n",
    "            (df2['start'] <= addr) &\n",
    "            (df2['end'] > addr)\n",
    "            #(df2['start_addr'] <= addr) &\n",
    "            #(df2['end_addr'] >= addr)\n",
    "            ]\n",
    "    if not matches.empty:\n",
    "        return matches.iloc[0]['rno'].astype(int) # if multiple matches, take the first\n",
    "    else:\n",
    "        #print(\"Failed! time {} addr {}\".format(time,addr))\n",
    "        #exit()\n",
    "        return None\n",
    "\n",
    "# Prepare a df for given PEBS sample file\n",
    "def prepare_pebs_df(file):\n",
    "    # Read the file line by line\n",
    "    with open(file) as f:\n",
    "        rows = [line.strip().split() for line in f if line.strip()]\n",
    "\n",
    "    # Find the maximum number of columns in any row\n",
    "    max_cols = max(len(row) for row in rows)\n",
    "\n",
    "    # Pad each row so all have the same length\n",
    "    #padded_rows = [row + [np.nan]*(max_cols - len(row)) for row in rows]\n",
    "\n",
    "    # Function to pad each row with the last recorded value\n",
    "    def pad_row(row, target_length):\n",
    "        if len(row) < target_length:\n",
    "            last_value = row[-1]\n",
    "            # Extend the row with the last_value until it reaches the target length\n",
    "            row = row + [last_value] * (target_length - len(row))\n",
    "        return row\n",
    "\n",
    "    # Pad each row accordingly\n",
    "    padded_rows = [pad_row(row, max_cols) for row in rows]\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(padded_rows)\n",
    "\n",
    "    # Rename columns: first column as 'PageFrame' and remaining as 'Epoch1', 'Epoch2', ...\n",
    "    df.rename(columns={0: \"PageFrame\"}, inplace=True)\n",
    "    df.columns = [\"PageFrame\"] + [f\"Epoch_{i}\" for i in range(1, max_cols)]\n",
    "\n",
    "    df[\"PageFrame\"] = df[\"PageFrame\"].apply(lambda x: hex(int(x, 16) << 21))\n",
    "\n",
    "    # Convert epoch columns to numeric\n",
    "    for col in df.columns[1:]:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "\n",
    "    # Set PageFrame as index for easier time-series operations\n",
    "    df.set_index(\"PageFrame\", inplace=True)\n",
    "\n",
    "    df = df.copy() # Improves performance? df is sparse otherwise\n",
    "\n",
    "    # Compute the deltas across epochs\n",
    "    delta_df = df.diff(axis=1)\n",
    "\n",
    "    # For the first epoch, fill NaN with the original epoch value\n",
    "    first_epoch = df.columns[0]\n",
    "    delta_df[first_epoch] = df[first_epoch]\n",
    "\n",
    "    # Reorder columns to ensure the first epoch is first\n",
    "    delta_df = delta_df[df.columns]\n",
    "\n",
    "    # Optional: Convert column names to a numeric index if desired\n",
    "    # For plotting purposes, we can remove the 'Epoch_' prefix and convert to int\n",
    "    delta_df.columns = [int(col.replace(\"Epoch_\", \"\"))*0.5 for col in delta_df.columns]\n",
    "\n",
    "    # If we want to use plt instead of sns, melt df into long form\n",
    "    df_long = (\n",
    "        delta_df\n",
    "        .reset_index()\n",
    "        .melt(id_vars=[\"PageFrame\"], var_name=\"epoch\", value_name=\"value\")\n",
    "    )\n",
    "    df_long[\"PageFrame\"] = df_long[\"PageFrame\"].apply(lambda x: int(x,16))\n",
    "    return df_long\n",
    "\n",
    "    return delta_df\n",
    "\n",
    "def get_reuse_distance_df(df):\n",
    "    df_zero_streak_sorted = df.sort_values(by=['PageFrame', 'epoch']).reset_index(drop=True)\n",
    "    \n",
    "    # Container for results\n",
    "    results = []\n",
    "    \n",
    "    # Group by PageFrame\n",
    "    for pf, group in df_zero_streak_sorted.groupby('PageFrame'):\n",
    "        # Mark where value == 0\n",
    "        zero_mask = group['value'] == 0\n",
    "    \n",
    "        # Identify start of new streaks using the change in zero_mask\n",
    "        streak_id = (zero_mask != zero_mask.shift()).cumsum()\n",
    "    \n",
    "        # For value == 0 streaks only, compute their lengths\n",
    "        zero_streaks = group[zero_mask].groupby(streak_id).size()\n",
    "    \n",
    "        # Get the max streak length (0 if none)\n",
    "        max_streak = zero_streaks.max() if not zero_streaks.empty else 0\n",
    "    \n",
    "        results.append({'PageFrame': pf, 'reuse_distance': max_streak})\n",
    "    \n",
    "    # Create a new dataframe\n",
    "    streak_df = pd.DataFrame(results)\n",
    "    return streak_df\n",
    "\n",
    "def calculate_duty_cycle(df):\n",
    "    # Calculate Duty Cycle\n",
    "    non_zero_df = df[df['value'] != 0]\n",
    "    counts = non_zero_df.groupby('PageFrame').size()\n",
    "    counts.name = 'duty_cycle'\n",
    "    df = df.merge(counts, on='PageFrame', how='left')\n",
    "    df['duty_cycle'] = df['duty_cycle'].fillna(0).astype(int)\n",
    "    df['duty_cycle_sample_count'] = len(df['epoch'].unique())\n",
    "    df['duty_cycle_percent'] = (df['duty_cycle'] / len(df['epoch'].unique())*100).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb81a9f5-9688-4c26-9b6d-29ee9c300805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch  rno            start              end     inode  \\\n",
      "0       0    0   93824992231424   93824992243712  92012753   \n",
      "1       0    1   93824992243712   93824992268288  92012753   \n",
      "2       0    2   93824992268288   93824992276480  92012753   \n",
      "3       0    3   93824992276480   93824992280576  92012753   \n",
      "4       0    4   93824992280576   93824992284672  92012753   \n",
      "..    ...  ...              ...              ...       ...   \n",
      "66      0   37  140737353895936  140737354067968    246279   \n",
      "67      0   38  140737354067968  140737354113024    246279   \n",
      "68      0   39  140737354117120  140737354125312    246279   \n",
      "69      0   40  140737354125312  140737354133504    246279   \n",
      "70      0   41  140737488216064  140737488351232         0   \n",
      "\n",
      "                                             pathname  size  rss_kb  pss_kb  \\\n",
      "0   /mydata/workloads/MERCI/4_performance_evaluati...    12      12      12   \n",
      "1   /mydata/workloads/MERCI/4_performance_evaluati...    24      24      24   \n",
      "2   /mydata/workloads/MERCI/4_performance_evaluati...     8       8       8   \n",
      "3   /mydata/workloads/MERCI/4_performance_evaluati...     4       4       4   \n",
      "4   /mydata/workloads/MERCI/4_performance_evaluati...     4       4       4   \n",
      "..                                                ...   ...     ...     ...   \n",
      "66     /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2   168     168       4   \n",
      "67     /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2    44      40       1   \n",
      "68     /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2     8       8       8   \n",
      "69     /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2     8       8       8   \n",
      "70                                            [stack]   132      20      20   \n",
      "\n",
      "    pss_dirty  referenced  \n",
      "0           0          12  \n",
      "1           0          24  \n",
      "2           0           8  \n",
      "3           4           4  \n",
      "4           4           4  \n",
      "..        ...         ...  \n",
      "66          0         168  \n",
      "67          0          40  \n",
      "68          8           8  \n",
      "69          8           8  \n",
      "70         20          20  \n",
      "\n",
      "[71 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "vma_df = (pd.read_csv('../../results/results_vma_cluster/eval_baseline_memory_regions_smap_deduplicated.csv'))\n",
    "\n",
    "next_rno = vma_df['rno'].max() + 1\n",
    "\n",
    "vma_df['start'] = vma_df['start'].apply(lambda x: int(x,16))\n",
    "vma_df['end'] = vma_df['end'].apply(lambda x: int(x,16))\n",
    "print(vma_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb1ddf5c-292f-4e66-9f93-b703e32a871c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch  rno            start              end  inode pathname     size  \\\n",
      "13     30    6  140727979667456  140736541978624      0      NaN  8361632   \n",
      "\n",
      "     rss_kb   pss_kb  pss_dirty  referenced  \n",
      "13  8098984  8098984    8098984     8098984  \n",
      "    epoch  rno            start              end  inode  pathname     size  \\\n",
      "13     30   42  140727979667456  140729053409280      0       NaN  1048576   \n",
      "13     30   43  140729053409280  140730127151104      0       NaN  1048576   \n",
      "13     30   44  140730127151104  140731200892928      0       NaN  1048576   \n",
      "13     30   45  140731200892928  140732274634752      0       NaN  1048576   \n",
      "13     30   46  140732274634752  140733348376576      0       NaN  1048576   \n",
      "13     30   47  140733348376576  140734422118400      0       NaN  1048576   \n",
      "13     30   48  140734422118400  140735495860224      0       NaN  1048576   \n",
      "13     30   49  140735495860224  140736541978624      0       NaN  1021600   \n",
      "\n",
      "     rss_kb   pss_kb  pss_dirty  referenced  \n",
      "13  8098984  8098984    8098984     8098984  \n",
      "13  8098984  8098984    8098984     8098984  \n",
      "13  8098984  8098984    8098984     8098984  \n",
      "13  8098984  8098984    8098984     8098984  \n",
      "13  8098984  8098984    8098984     8098984  \n",
      "13  8098984  8098984    8098984     8098984  \n",
      "13  8098984  8098984    8098984     8098984  \n",
      "13  8098984  8098984    8098984     8098984  \n"
     ]
    }
   ],
   "source": [
    "# Get only vma with no pathname (anon region) and a size over 2 MB\n",
    "filtered_vma_df = (vma_df[pd.isna(vma_df['pathname']) & (vma_df['size'] >= (1<<21))])\n",
    "#filtered_vma_df = vma_df\n",
    "print(filtered_vma_df)\n",
    "def split_large_rows(df, next_rno, size_threshold=(1<<20)):\n",
    "    new_rows = []\n",
    "    #next_rno = df['rno'].max() + 1  # Start new rno from max + 1\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if row['size'] > size_threshold:\n",
    "            # Calculate number of chunks needed\n",
    "            num_chunks = int(row['size'] // size_threshold)\n",
    "            last_chunk_size = row['size'] % size_threshold\n",
    "\n",
    "            # Split into chunks\n",
    "            start = row['start']\n",
    "            for i in range(num_chunks):\n",
    "                new_row = row.copy()\n",
    "                new_row['rno'] = next_rno\n",
    "                new_row['start'] = start\n",
    "                new_row['end'] = start + size_threshold * (1<<10)\n",
    "                new_row['size'] = size_threshold\n",
    "                new_rows.append(new_row)\n",
    "                start += size_threshold * (1<<10)\n",
    "                next_rno += 1\n",
    "\n",
    "            # Last chunk (if any remainder)\n",
    "            if last_chunk_size > 0:\n",
    "                new_row = row.copy()\n",
    "                new_row['rno'] = next_rno\n",
    "                new_row['start'] = start\n",
    "                new_row['end'] = start + last_chunk_size * (1<<10)\n",
    "                new_row['size'] = last_chunk_size\n",
    "                new_rows.append(new_row)\n",
    "                next_rno += 1\n",
    "        else:\n",
    "            new_rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "split_vma_df = (split_large_rows(filtered_vma_df, next_rno))\n",
    "print(split_vma_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1770f8f2-498e-4c53-ba66-77190097b129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               PageFrame  epoch  value  time_bin\n",
      "0                4194304    0.5      0         0\n",
      "1                6291456    0.5      0         0\n",
      "2                8388608    0.5      0         0\n",
      "3               10485760    0.5      0         0\n",
      "4               12582912    0.5      0         0\n",
      "...                  ...    ...    ...       ...\n",
      "5007615  140737347846144  227.0      0        22\n",
      "5007616  140737349943296  227.0      0        22\n",
      "5007617  140737352040448  227.0      0        22\n",
      "5007618  140737484161024  227.0      0        22\n",
      "5007619  140737486258176  227.0      0        22\n",
      "\n",
      "[5007620 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read in pebs data and bin in N second intervals\n",
    "N = 10\n",
    "df = prepare_pebs_df('../../results/results_vma_cluster/merci_merci_samples.dat')\n",
    "df['time_bin'] = (df['epoch'] // N).astype(int)\n",
    "print(df)\n",
    "dfs_by_interval = {\n",
    "    f\"{N * bin}s_to_{N * (bin + 1)}s\": group.drop(columns='time_bin')\n",
    "    for bin, group in df.groupby('time_bin')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3dcfa6-31f5-4d5b-a08e-7f52f5ff4b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#page_stat_df = None\n",
    "labeled_dfs = []\n",
    "i = 0\n",
    "for df in dfs_by_interval.values():\n",
    "    #if df['epoch'].min() < 70:\n",
    "    #    continue\n",
    "    time_bin_df = df.copy()\n",
    "    duty_df = calculate_duty_cycle(time_bin_df)\n",
    "    duty_df = duty_df.drop_duplicates(subset='PageFrame')[['PageFrame', 'duty_cycle', 'duty_cycle_sample_count', 'duty_cycle_percent']]\n",
    "    #duty_avg_df = (df.drop(columns=['epoch', 'duty_cycle_sample_count', 'duty_cycle', 'value']).groupby('PageFrame', as_index=False).mean().astype(int))\n",
    "\n",
    "    #print(duty_df)\n",
    "    streak_df = get_reuse_distance_df(time_bin_df)\n",
    "    time_bin_df = time_bin_df.merge(streak_df, on='PageFrame', how='left')\n",
    "    \n",
    "    #print(streak_df)\n",
    "    #break\n",
    "    page_stat_df = time_bin_df.groupby('PageFrame').agg(\n",
    "        {\n",
    "            'value': ['mean', 'std', 'min', 'max'],\n",
    "            'reuse_distance': ['mean']\n",
    "        }\n",
    "    )\n",
    "    #print('time bin...---------')\n",
    "    #print(time_bin_df)\n",
    "    #print('page_stat ..........')\n",
    "    #print(page_stat_df)\n",
    "\n",
    "    # Combine duty cycle info with access statistics\n",
    "    page_stat_df.columns = ['_'.join(col) for col in page_stat_df.columns]\n",
    "    page_stat_df = page_stat_df.merge(duty_df, on='PageFrame', how='left')\n",
    "    page_stat_df = page_stat_df.reset_index(drop=True)\n",
    "\n",
    "    # Apply region numbers, do this last on smaller aggregated data set because it takes a while.\n",
    "    page_stat_df['rno'] = page_stat_df.apply(lambda row: find_region_id(row, split_vma_df), axis=1)\n",
    "    #page_stat_df['rno'] = page_stat_df['rno'].fillna(-1)\n",
    "    #print(page_stat_df)\n",
    "    page_stat_df = page_stat_df.dropna().reset_index(drop=True)\n",
    "    page_stat_df['rno'] = page_stat_df['rno'].astype(int)\n",
    "\n",
    "    #print(page_stat_df)\n",
    "    #print('-----')\n",
    "    clustered_df = apply_cluster(page_stat_df.copy())\n",
    "\n",
    "    time_bin_df = time_bin_df.merge(\n",
    "        clustered_df[['PageFrame', 'cluster']].drop_duplicates('PageFrame'),\n",
    "        on='PageFrame',\n",
    "        how='left'\n",
    "    )\n",
    "    time_bin_df = time_bin_df.dropna()\n",
    "    labeled_dfs.append(time_bin_df)\n",
    "    print(i)\n",
    "    #if i > 1:\n",
    "    #    break\n",
    "    i+=1\n",
    "\n",
    "    #if time_bin_df['epoch'].min() > 70:\n",
    "    #    break\n",
    "\n",
    "print(\"Labeled DFS==============\")\n",
    "#print(page_stat_df)\n",
    "print(labeled_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeaa19c-34b5-46a7-97c5-1c0872a2b6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874eea98-9174-441e-afab-b87ca51ab56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat(labeled_dfs, ignore_index=True)\n",
    "print(final_df)\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.scatter(final_df['epoch'], final_df['PageFrame'], c=final_df['cluster'], s=50, edgecolor='none', rasterized=True, alpha=0.7, marker='.')\n",
    "\n",
    "#df.plot.scatter(x='x', y='y', c='cluster', colormap='viridis', label='Clusters')\n",
    "#final_df.plot.scatter(x ='epoch', y='PageFrame', c='cluster',label='Clusters', s=50, edgecolor='none', rasterized=True, alpha=0.7, marker='.')\n",
    "\n",
    "#plt.legend()  # sometimes required explicitly\n",
    "#plt.show()\n",
    "\n",
    "#plt.scatter(df['time'], df['address'], s=50, edgecolor='none', rasterized=True, alpha=0.7, marker='.')\n",
    "xmin = final_df['epoch'].min()\n",
    "xmax = final_df['epoch'].max()\n",
    "ymin = final_df['PageFrame'].min() + (1<<30)\n",
    "#plt.hlines(y=ymin, xmin=xmin, xmax=xmax, colors='red', linestyles='dashed')\n",
    "ax = plt.gca()\n",
    "\n",
    "# 1) Define a hex‐formatter: takes a float x and returns e.g. '0x1a3f'\n",
    "hex_formatter = FuncFormatter(lambda x, pos: hex(int(x)))\n",
    "\n",
    "# 2) Install it on the y‐axis\n",
    "ax.yaxis.set_major_formatter(hex_formatter)\n",
    "ax.invert_yaxis()\n",
    "#sns.scatterplot(data=time_bin_df1.dropna(), y='PageFrame', x='epoch', hue='cluster', palette=sns.color_palette(\"tab10\"))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa51e32d-4c78-42db-8b27-0b40ba97cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labeled_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd9087-e2a2-4519-8790-22bc35222e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_bin_df['cluster'] = clustered\n",
    "#time_bin_df['cluster'] = time_bin_df['PageFrame'].map(clustered_df.set_index('PageFrame')['cluster'])\n",
    "time_bin_df1 = time_bin_df.merge(\n",
    "    clustered_df[['PageFrame', 'cluster']].drop_duplicates('PageFrame'),\n",
    "    on='PageFrame',\n",
    "    how='left'\n",
    ")\n",
    "#print(clustered_df)\n",
    "#time_bin_df1['cluster'] = (time_bin_df1['cluster'].fillna(-1))\n",
    "time_bin_df1 = (time_bin_df1.dropna())\n",
    "print(time_bin_df1)\n",
    "#plt.scatter(time_bin_df1['epoch'], time_bin_df1['PageFrame'], c=time_bin_df1['cluster'])\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.scatter(time_bin_df1['epoch'], time_bin_df1['PageFrame'], c=time_bin_df1['cluster'], s=50, edgecolor='none', rasterized=True, alpha=0.7, marker='.')\n",
    "#plt.scatter(df['time'], df['address'], s=50, edgecolor='none', rasterized=True, alpha=0.7, marker='.')\n",
    "xmin = df['epoch'].min()\n",
    "xmax = df['epoch'].max()\n",
    "ymin = time_bin_df1['PageFrame'].min() + (1<<30)\n",
    "plt.hlines(y=ymin, xmin=xmin, xmax=xmax, colors='red', linestyles='dashed')\n",
    "ax = plt.gca()\n",
    "\n",
    "# 1) Define a hex‐formatter: takes a float x and returns e.g. '0x1a3f'\n",
    "hex_formatter = FuncFormatter(lambda x, pos: hex(int(x)))\n",
    "\n",
    "# 2) Install it on the y‐axis\n",
    "ax.yaxis.set_major_formatter(hex_formatter)\n",
    "ax.invert_yaxis()\n",
    "#sns.scatterplot(data=time_bin_df1.dropna(), y='PageFrame', x='epoch', hue='cluster', palette=sns.color_palette(\"tab10\"))\n",
    "plt.show()\n",
    "#page_stat_df['rno'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dffaf56-f994-4ba6-8cc4-f65dea78f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labeled_dfs[1])\n",
    "\n",
    "sns.scatterplot(data=clustered_df, y=0, x=1, hue='cluster', palette=sns.color_palette(\"tab10\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176511b4-aed9-4007-b7ce-dbda5e61a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_pebs_df(file):\n",
    "    # Read the file line by line\n",
    "    with open(file) as f:\n",
    "        rows = [line.strip().split() for line in f if line.strip()]\n",
    "\n",
    "    # Find the maximum number of columns in any row\n",
    "    max_cols = max(len(row) for row in rows)\n",
    "\n",
    "    # Pad each row so all have the same length\n",
    "    #padded_rows = [row + [np.nan]*(max_cols - len(row)) for row in rows]\n",
    "\n",
    "    # Function to pad each row with the last recorded value\n",
    "    def pad_row(row, target_length):\n",
    "        if len(row) < target_length:\n",
    "            last_value = row[-1]\n",
    "            # Extend the row with the last_value until it reaches the target length\n",
    "            row = row + [last_value] * (target_length - len(row))\n",
    "        return row\n",
    "\n",
    "    # Pad each row accordingly\n",
    "    padded_rows = [pad_row(row, max_cols) for row in rows]\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(padded_rows)\n",
    "\n",
    "    # Rename columns: first column as 'PageFrame' and remaining as 'Epoch1', 'Epoch2', ...\n",
    "    df.rename(columns={0: \"PageFrame\"}, inplace=True)\n",
    "    df.columns = [\"PageFrame\"] + [f\"Epoch_{i}\" for i in range(1, max_cols)]\n",
    "\n",
    "    df[\"PageFrame\"] = df[\"PageFrame\"].apply(lambda x: hex(int(x, 16) << 21))\n",
    "\n",
    "    # Convert epoch columns to numeric\n",
    "    for col in df.columns[1:]:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "\n",
    "    # Set PageFrame as index for easier time-series operations\n",
    "    df.set_index(\"PageFrame\", inplace=True)\n",
    "\n",
    "    df = df.copy() # Improves performance? df is sparse otherwise\n",
    "\n",
    "    # Compute the deltas across epochs\n",
    "    delta_df = df.diff(axis=1)\n",
    "\n",
    "    # For the first epoch, fill NaN with the original epoch value\n",
    "    first_epoch = df.columns[0]\n",
    "    delta_df[first_epoch] = df[first_epoch]\n",
    "\n",
    "    # Reorder columns to ensure the first epoch is first\n",
    "    delta_df = delta_df[df.columns]\n",
    "\n",
    "    # Optional: Convert column names to a numeric index if desired\n",
    "    # For plotting purposes, we can remove the 'Epoch_' prefix and convert to int\n",
    "    delta_df.columns = [int(col.replace(\"Epoch_\", \"\"))*0.5 for col in delta_df.columns]\n",
    "\n",
    "    # If we want to use plt instead of sns, melt df into long form\n",
    "    #df_long = (\n",
    "    #    delta_df\n",
    "    #    .reset_index()\n",
    "    #    .melt(id_vars=[\"PageFrame\"], var_name=\"epoch\", value_name=\"value\")\n",
    "    #)\n",
    "    #df_long[\"PageFrame\"] = df_long[\"PageFrame\"].apply(lambda x: int(x,16))\n",
    "    #return df_long\n",
    "\n",
    "    return delta_df\n",
    "\n",
    "def generate_pebs_figure(file, output_path):\n",
    "\n",
    "    output_file = output_path + \"_heatmap.png\"\n",
    "    print(\"Checking {}\".format(output_file))\n",
    "\n",
    "    if os.path.isfile(output_file):\n",
    "        print(\"Skipping {}\".format(output_file))\n",
    "        return\n",
    "\n",
    "    df = prepare_pebs_df(file)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    #sns.heatmap(df, cmap=\"viridis\", cbar=True, norm=LogNorm())\n",
    "\n",
    "    xmin = df['epoch'].min()\n",
    "    xmax = df['epoch'].max()\n",
    "    \n",
    "    # Draw a horizontal line at y = some_value\n",
    "    ymax = time_bin_df1['PageFrame'].max()\n",
    "    ymin = time_bin_df1['PageFrame'].min()\n",
    "    plt.hlines(y=ymax, xmin=xmin, xmax=xmax, colors='red', linestyles='dashed')\n",
    "    plt.hlines(y=ymin, xmin=xmin, xmax=xmax, colors='red', linestyles='dashed')\n",
    "\n",
    "    df = df[df['PageFrame'] >= ymin]\n",
    "    df = df[df['PageFrame'] <= ymax]\n",
    "    # If we want to use plt instead of sns\n",
    "    plt.scatter(df['epoch'], df['PageFrame'], c=df['value'], s=50, norm=LogNorm(), edgecolor='none', rasterized=True, alpha=0.7, marker='.')\n",
    "\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ## 1) Define a hex‐formatter: takes a float x and returns e.g. '0x1a3f'\n",
    "    hex_formatter = FuncFormatter(lambda x, pos: hex(int(x)))\n",
    "\n",
    "    ## 2) Install it on the y‐axis\n",
    "    ax.yaxis.set_major_formatter(hex_formatter)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Page Frame\")\n",
    "    plt.title(file + \": PEBS\")\n",
    "    plt.show()\n",
    "    #plt.savefig(output_file, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "generate_pebs_figure('../../results/results_vma_cluster/merci_merci_samples.dat', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d9c4c-186c-4869-9fc8-abec4079a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#vma_stat_df = page_stat_df.groupby('rno').agg(\n",
    "#        {\n",
    "##            'duty_cycle_percent': ['count', 'mean', 'std', 'min', 'max'],\n",
    "#            'reuse_distance': ['count', 'mean', 'std', 'min', 'max']\n",
    "#        }\n",
    "#    )\n",
    "\n",
    "#filtered_vma_df = vma_df\n",
    "#filtered_vma_df = vma_df[vma_df['pathname'] == pd.NA]\n",
    "\n",
    "#vma_stat_df.columns = ['_'.join(col) for col in vma_stat_df.columns]\n",
    "#vma_stat_df = vma_stat_df.reset_index()\n",
    "#vma_stat_df = pd.merge(vma_stat_df, filtered_vma_df, on='rno', how='left')\n",
    "#vma_stat_df['rss_kb_percent'] = vma_stat_df['rss_kb'] / vma_stat_df['size']\n",
    "#vma_stat_df['pss_kb_dirty_percent'] = vma_stat_df['pss_dirty'] / vma_stat_df['pss_kb']\n",
    "#print(vma_stat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51e6d8-d77e-40f7-ab52-8fcfb031f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out VMAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3a403-44a2-4d08-a350-62b76d046603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vma_stat_df = vma_stat_df.loc[vma_stat_df.groupby('rno')['size'].idxmax()]\n",
    "#print(vma_stat_df.reset_index(drop=True).drop(columns=['epoch', 'start', 'end', 'inode', 'pathname', 'rss_kb', 'pss_kb', 'pss_dirty', 'referenced']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0267b5c2-9e49-472e-8e71-18e3266b02f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clustered_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
