{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fddbf330-399a-4939-b345-ea2a28446cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Used to accelerate plotting DAMON figures.\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "\n",
    "from matplotlib.colors import LogNorm, hsv_to_rgb\n",
    "\n",
    "def find_region_id(row, df2):\n",
    "    #print(row)\n",
    "    #time = row['time']\n",
    "    addr = row['PageFrame']\n",
    "    matches = df2[\n",
    "            (df2['start'] <= addr) &\n",
    "            (df2['end'] >= addr)\n",
    "            #(df2['start_addr'] <= addr) &\n",
    "            #(df2['end_addr'] >= addr)\n",
    "            ]\n",
    "    if not matches.empty:\n",
    "        return matches.iloc[0]['rno'].astype(int)  # if multiple matches, take the first\n",
    "    else:\n",
    "        #print(\"Failed! time {} addr {}\".format(time,addr))\n",
    "        #exit()\n",
    "        return None\n",
    "\n",
    "# Prepare a df for given PEBS sample file\n",
    "def prepare_pebs_df(file):\n",
    "    # Read the file line by line\n",
    "    with open(file) as f:\n",
    "        rows = [line.strip().split() for line in f if line.strip()]\n",
    "\n",
    "    # Find the maximum number of columns in any row\n",
    "    max_cols = max(len(row) for row in rows)\n",
    "\n",
    "    # Pad each row so all have the same length\n",
    "    #padded_rows = [row + [np.nan]*(max_cols - len(row)) for row in rows]\n",
    "\n",
    "    # Function to pad each row with the last recorded value\n",
    "    def pad_row(row, target_length):\n",
    "        if len(row) < target_length:\n",
    "            last_value = row[-1]\n",
    "            # Extend the row with the last_value until it reaches the target length\n",
    "            row = row + [last_value] * (target_length - len(row))\n",
    "        return row\n",
    "\n",
    "    # Pad each row accordingly\n",
    "    padded_rows = [pad_row(row, max_cols) for row in rows]\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(padded_rows)\n",
    "\n",
    "    # Rename columns: first column as 'PageFrame' and remaining as 'Epoch1', 'Epoch2', ...\n",
    "    df.rename(columns={0: \"PageFrame\"}, inplace=True)\n",
    "    df.columns = [\"PageFrame\"] + [f\"Epoch_{i}\" for i in range(1, max_cols)]\n",
    "\n",
    "    df[\"PageFrame\"] = df[\"PageFrame\"].apply(lambda x: hex(int(x, 16) << 21))\n",
    "\n",
    "    # Convert epoch columns to numeric\n",
    "    for col in df.columns[1:]:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "\n",
    "    # Set PageFrame as index for easier time-series operations\n",
    "    df.set_index(\"PageFrame\", inplace=True)\n",
    "\n",
    "    df = df.copy() # Improves performance? df is sparse otherwise\n",
    "\n",
    "    # Compute the deltas across epochs\n",
    "    delta_df = df.diff(axis=1)\n",
    "\n",
    "    # For the first epoch, fill NaN with the original epoch value\n",
    "    first_epoch = df.columns[0]\n",
    "    delta_df[first_epoch] = df[first_epoch]\n",
    "\n",
    "    # Reorder columns to ensure the first epoch is first\n",
    "    delta_df = delta_df[df.columns]\n",
    "\n",
    "    # Optional: Convert column names to a numeric index if desired\n",
    "    # For plotting purposes, we can remove the 'Epoch_' prefix and convert to int\n",
    "    delta_df.columns = [int(col.replace(\"Epoch_\", \"\"))*0.5 for col in delta_df.columns]\n",
    "\n",
    "    # If we want to use plt instead of sns, melt df into long form\n",
    "    df_long = (\n",
    "        delta_df\n",
    "        .reset_index()\n",
    "        .melt(id_vars=[\"PageFrame\"], var_name=\"epoch\", value_name=\"value\")\n",
    "    )\n",
    "    df_long[\"PageFrame\"] = df_long[\"PageFrame\"].apply(lambda x: int(x,16))\n",
    "    return df_long\n",
    "\n",
    "    return delta_df\n",
    "\n",
    "def get_reuse_distance_df(df):\n",
    "    df_zero_streak_sorted = df.sort_values(by=['PageFrame', 'epoch']).reset_index(drop=True)\n",
    "    \n",
    "    # Container for results\n",
    "    results = []\n",
    "    \n",
    "    # Group by PageFrame\n",
    "    for pf, group in df_zero_streak_sorted.groupby('PageFrame'):\n",
    "        # Mark where value == 0\n",
    "        zero_mask = group['value'] == 0\n",
    "    \n",
    "        # Identify start of new streaks using the change in zero_mask\n",
    "        streak_id = (zero_mask != zero_mask.shift()).cumsum()\n",
    "    \n",
    "        # For value == 0 streaks only, compute their lengths\n",
    "        zero_streaks = group[zero_mask].groupby(streak_id).size()\n",
    "    \n",
    "        # Get the max streak length (0 if none)\n",
    "        max_streak = zero_streaks.max() if not zero_streaks.empty else 0\n",
    "    \n",
    "        results.append({'PageFrame': pf, 'reuse_distance': max_streak})\n",
    "    \n",
    "    # Create a new dataframe\n",
    "    streak_df = pd.DataFrame(results)\n",
    "    return streak_df\n",
    "\n",
    "def calculate_duty_cycle(df):\n",
    "    # Calculate Duty Cycle\n",
    "    non_zero_df = df[df['value'] != 0]\n",
    "    counts = non_zero_df.groupby('PageFrame').size()\n",
    "    counts.name = 'duty_cycle'\n",
    "    df = df.merge(counts, on='PageFrame', how='left')\n",
    "    df['duty_cycle'] = df['duty_cycle'].fillna(0).astype(int)\n",
    "    df['duty_cycle_sample_count'] = len(df['epoch'].unique())\n",
    "    df['duty_cycle_percent'] = (df['duty_cycle'] / len(df['epoch'].unique())*100).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb81a9f5-9688-4c26-9b6d-29ee9c300805",
   "metadata": {},
   "outputs": [],
   "source": [
    "vma_df = (pd.read_csv('../../smap_deduplicated.csv'))\n",
    "\n",
    "vma_df['start'] = vma_df['start'].apply(lambda x: int(x,16))\n",
    "vma_df['end'] = vma_df['end'].apply(lambda x: int(x,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb1ddf5c-292f-4e66-9f93-b703e32a871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only vma with no pathname (anon region) and a size over 2 MB\n",
    "filtered_vma_df = (vma_df[pd.isna(vma_df['pathname']) & (vma_df['size'] >= (1<<21))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1770f8f2-498e-4c53-ba66-77190097b129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               PageFrame  epoch  value  time_bin\n",
      "0                4194304    0.5      0         0\n",
      "1                6291456    0.5      0         0\n",
      "2                8388608    0.5      0         0\n",
      "3               10485760    0.5      0         0\n",
      "4               12582912    0.5      0         0\n",
      "...                  ...    ...    ...       ...\n",
      "2733219  140737345748992  266.5      0        26\n",
      "2733220  140737347846144  266.5      3        26\n",
      "2733221  140737349943296  266.5     11        26\n",
      "2733222  140737352040448  266.5     12        26\n",
      "2733223  140737486258176  266.5      0        26\n",
      "\n",
      "[2733224 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read in pebs data and bin in N second intervals\n",
    "N = 10\n",
    "df = prepare_pebs_df('../../results/results_gapbs_vma/gapbs_cc_sv_samples.dat')\n",
    "df['time_bin'] = (df['epoch'] // N).astype(int)\n",
    "print(df)\n",
    "dfs_by_interval = {\n",
    "    f\"{N * bin}s_to_{N * (bin + 1)}s\": group.drop(columns='time_bin')\n",
    "    for bin, group in df.groupby('time_bin')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f3dcfa6-31f5-4d5b-a08e-7f52f5ff4b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      index        PageFrame  value_mean   value_std  value_min  value_max  \\\n",
      "301     301  140727233282048    0.000000    0.000000          0          0   \n",
      "302     302  140727235379200    0.000000    0.000000          0          0   \n",
      "303     303  140727237476352    0.000000    0.000000          0          0   \n",
      "304     304  140727239573504    0.000000    0.000000          0          0   \n",
      "305     305  140727241670656    0.000000    0.000000          0          0   \n",
      "...     ...              ...         ...         ...        ...        ...   \n",
      "5119   5119  140737337360384  127.263158  264.981308          0        854   \n",
      "5120   5120  140737339457536  110.000000  251.484702          0        862   \n",
      "5121   5121  140737341554688  149.421053  325.104140          0       1163   \n",
      "5122   5122  140737343651840  212.368421  430.572256          0       1531   \n",
      "5123   5123  140737345748992    1.473684    2.776257          0          8   \n",
      "\n",
      "      reuse_distance_mean  reuse_distance_std  reuse_distance_min  \\\n",
      "301                  19.0                 0.0                  19   \n",
      "302                  19.0                 0.0                  19   \n",
      "303                  19.0                 0.0                  19   \n",
      "304                  19.0                 0.0                  19   \n",
      "305                  19.0                 0.0                  19   \n",
      "...                   ...                 ...                 ...   \n",
      "5119                 13.0                 0.0                  13   \n",
      "5120                 13.0                 0.0                  13   \n",
      "5121                 13.0                 0.0                  13   \n",
      "5122                 13.0                 0.0                  13   \n",
      "5123                  3.0                 0.0                   3   \n",
      "\n",
      "      reuse_distance_max  duty_cycle  duty_cycle_sample_count  \\\n",
      "301                   19           0                       19   \n",
      "302                   19           0                       19   \n",
      "303                   19           0                       19   \n",
      "304                   19           0                       19   \n",
      "305                   19           0                       19   \n",
      "...                  ...         ...                      ...   \n",
      "5119                  13           5                       19   \n",
      "5120                  13           5                       19   \n",
      "5121                  13           5                       19   \n",
      "5122                  13           5                       19   \n",
      "5123                   3           6                       19   \n",
      "\n",
      "      duty_cycle_percent  rno  \n",
      "301                    0    6  \n",
      "302                    0    6  \n",
      "303                    0    6  \n",
      "304                    0    6  \n",
      "305                    0    6  \n",
      "...                  ...  ...  \n",
      "5119                  26    6  \n",
      "5120                  26    6  \n",
      "5121                  26    6  \n",
      "5122                  26    6  \n",
      "5123                  31    6  \n",
      "\n",
      "[4823 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "for df in dfs_by_interval.values():\n",
    "    time_bin_df = df.copy()\n",
    "    duty_df = calculate_duty_cycle(time_bin_df)\n",
    "    duty_df = duty_df.drop_duplicates(subset='PageFrame')[['PageFrame', 'duty_cycle', 'duty_cycle_sample_count', 'duty_cycle_percent']]\n",
    "    #duty_avg_df = (df.drop(columns=['epoch', 'duty_cycle_sample_count', 'duty_cycle', 'value']).groupby('PageFrame', as_index=False).mean().astype(int))\n",
    "\n",
    "    #print(duty_df)\n",
    "    streak_df = get_reuse_distance_df(time_bin_df)\n",
    "    time_bin_df = time_bin_df.merge(streak_df, on='PageFrame', how='left')\n",
    "    \n",
    "    #print(streak_df)\n",
    "    #break\n",
    "    page_stat_df = time_bin_df.groupby('PageFrame').agg(\n",
    "        {\n",
    "            'value': ['mean', 'std', 'min', 'max'],\n",
    "            'reuse_distance': ['mean', 'std', 'min', 'max']\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Combine duty cycle info with access statistics\n",
    "    page_stat_df.columns = ['_'.join(col) for col in page_stat_df.columns]\n",
    "    page_stat_df = page_stat_df.merge(duty_df, on='PageFrame', how='left')\n",
    "    page_stat_df = page_stat_df.reset_index()\n",
    "\n",
    "    # Apply region numbers, do this last on smaller aggregated data set because it takes a while.\n",
    "    page_stat_df['rno'] = page_stat_df.apply(lambda row: find_region_id(row, filtered_vma_df), axis=1)\n",
    "    page_stat_df = page_stat_df.dropna()\n",
    "    page_stat_df['rno'] = page_stat_df['rno'].astype(int)\n",
    "    print(page_stat_df)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2707cd31-2c1a-4771-92e6-37ac1b9a8dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             PageFrame epoch  value  rno\n",
      "301    140727233282048   0.5      0    6\n",
      "302    140727235379200   0.5      0    6\n",
      "303    140727237476352   0.5      0    6\n",
      "304    140727239573504   0.5      0    6\n",
      "305    140727241670656   0.5      0    6\n",
      "...                ...   ...    ...  ...\n",
      "97423  140737337360384   9.5      0    6\n",
      "97424  140737339457536   9.5      0    6\n",
      "97425  140737341554688   9.5      0    6\n",
      "97426  140737343651840   9.5      0    6\n",
      "97427  140737345748992   9.5      0    6\n",
      "\n",
      "[91637 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_126271/1564837638.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['rno'] = df['rno'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Assign each page frame a VMA from out filtered VMA data set\n",
    "df['rno'] = df.apply(lambda row: find_region_id(row, filtered_vma_df), axis=1)\n",
    "df = df.dropna()\n",
    "df['rno'] = df['rno'].astype(int)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9edac96d-1311-4893-b7dd-09a7056fd998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            PageFrame  value_mean   value_std  value_min  value_max\n",
      "0     140727233282048    0.000000    0.000000          0          0\n",
      "1     140727235379200    0.000000    0.000000          0          0\n",
      "2     140727237476352    0.000000    0.000000          0          0\n",
      "3     140727239573504    0.000000    0.000000          0          0\n",
      "4     140727241670656    0.000000    0.000000          0          0\n",
      "...               ...         ...         ...        ...        ...\n",
      "4818  140737337360384  127.263158  264.981308          0        854\n",
      "4819  140737339457536  110.000000  251.484702          0        862\n",
      "4820  140737341554688  149.421053  325.104140          0       1163\n",
      "4821  140737343651840  212.368421  430.572256          0       1531\n",
      "4822  140737345748992    1.473684    2.776257          0          8\n",
      "\n",
      "[4823 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "page_stat_df = df.groupby('PageFrame').agg(\n",
    "        {\n",
    "            'value': ['mean', 'std', 'min', 'max'],\n",
    "            #'reuse_distance': ['count', 'mean', 'std', 'min', 'max']\n",
    "        }\n",
    "    )\n",
    "page_stat_df.columns = ['_'.join(col) for col in page_stat_df.columns]\n",
    "page_stat_df = page_stat_df.reset_index()\n",
    "print(page_stat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29df1218-49fa-414a-85bf-64bd58d2ca02",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['duty_cycle_sample_count', 'duty_cycle'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m duty_df = (df.drop(columns=[\u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mduty_cycle_sample_count\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mduty_cycle\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m'\u001b[39m]).groupby(\u001b[33m'\u001b[39m\u001b[33mPageFrame\u001b[39m\u001b[33m'\u001b[39m, as_index=\u001b[38;5;28;01mFalse\u001b[39;00m).mean().astype(\u001b[38;5;28mint\u001b[39m))\n\u001b[32m      2\u001b[39m page_stat_df = page_stat_df.merge(duty_df, on=\u001b[33m'\u001b[39m\u001b[33mPageFrame\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(page_stat_df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dataVis/lib/python3.13/site-packages/pandas/core/frame.py:5581\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5434\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5435\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5442\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5443\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5444\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5445\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5446\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5579\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5580\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5581\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().drop(\n\u001b[32m   5582\u001b[39m         labels=labels,\n\u001b[32m   5583\u001b[39m         axis=axis,\n\u001b[32m   5584\u001b[39m         index=index,\n\u001b[32m   5585\u001b[39m         columns=columns,\n\u001b[32m   5586\u001b[39m         level=level,\n\u001b[32m   5587\u001b[39m         inplace=inplace,\n\u001b[32m   5588\u001b[39m         errors=errors,\n\u001b[32m   5589\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dataVis/lib/python3.13/site-packages/pandas/core/generic.py:4788\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4786\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4787\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4788\u001b[39m         obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n\u001b[32m   4790\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4791\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dataVis/lib/python3.13/site-packages/pandas/core/generic.py:4830\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4828\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4829\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4830\u001b[39m         new_axis = axis.drop(labels, errors=errors)\n\u001b[32m   4831\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4833\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4834\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dataVis/lib/python3.13/site-packages/pandas/core/indexes/base.py:7070\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7068\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7069\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7070\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7071\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
      "\u001b[31mKeyError\u001b[39m: \"['duty_cycle_sample_count', 'duty_cycle'] not found in axis\""
     ]
    }
   ],
   "source": [
    "duty_df = (df.drop(columns=['epoch', 'duty_cycle_sample_count', 'duty_cycle', 'value']).groupby('PageFrame', as_index=False).mean().astype(int))\n",
    "page_stat_df = page_stat_df.merge(duty_df, on='PageFrame', how='left')\n",
    "print(page_stat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bd123f-8243-46d4-adb5-b7e23c8aed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "streak_df = get_reuse_distance_df(df)\n",
    "print(streak_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28601197-24d2-4eb4-be23-c1319549404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_stat_df = page_stat_df.merge(streak_df, on='PageFrame', how='left')\n",
    "print(page_stat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a555617-f966-4fae-b4a7-6c0a57c3b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def apply_cluster(page_stat_df):\n",
    "    scaler = StandardScaler()\n",
    "    features = page_stat_df.drop(columns=['PageFrame', 'rno'])\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    \n",
    "    pca_col = ['pc1', 'pc2']\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_df = pd.DataFrame(pca.fit_transform(scaled_features), columns=pca_col)\n",
    "    \n",
    "    k = 8\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    #kmeans.fit(scaled_features)\n",
    "    kmeans.fit(pca_df)\n",
    "    \n",
    "    page_stat_df['cluster'] = kmeans.labels_\n",
    "    page_stat_df_merged = (pd.concat([page_stat_df, pca_df], axis=1))\n",
    "    page_stat_df_merged['start_epoch'] = 0\n",
    "\n",
    "    # Return new df with cluster labels and pca values\n",
    "    return page_stat_df_merged\n",
    "\n",
    "clustered_df = apply_cluster(page_stat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dffaf56-f994-4ba6-8cc4-f65dea78f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clustered_df)\n",
    "\n",
    "sns.scatterplot(data=clustered_df, y='pc2', x='pc1', hue='cluster', palette=sns.color_palette(\"tab10\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d9c4c-186c-4869-9fc8-abec4079a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "vma_stat_df = page_stat_df.groupby('rno').agg(\n",
    "        {\n",
    "            'duty_cycle_percent': ['count', 'mean', 'std', 'min', 'max'],\n",
    "            'reuse_distance': ['count', 'mean', 'std', 'min', 'max']\n",
    "        }\n",
    "    )\n",
    "\n",
    "filtered_vma_df = vma_df\n",
    "filtered_vma_df = vma_df[vma_df['pathname'] == pd.NA]\n",
    "\n",
    "vma_stat_df.columns = ['_'.join(col) for col in vma_stat_df.columns]\n",
    "vma_stat_df = vma_stat_df.reset_index()\n",
    "vma_stat_df = pd.merge(vma_stat_df, filtered_vma_df, on='rno', how='left')\n",
    "vma_stat_df['rss_kb_percent'] = vma_stat_df['rss_kb'] / vma_stat_df['size']\n",
    "vma_stat_df['pss_kb_dirty_percent'] = vma_stat_df['pss_dirty'] / vma_stat_df['pss_kb']\n",
    "print(vma_stat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51e6d8-d77e-40f7-ab52-8fcfb031f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out VMAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3a403-44a2-4d08-a350-62b76d046603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vma_stat_df = vma_stat_df.loc[vma_stat_df.groupby('rno')['size'].idxmax()]\n",
    "#print(vma_stat_df.reset_index(drop=True).drop(columns=['epoch', 'start', 'end', 'inode', 'pathname', 'rss_kb', 'pss_kb', 'pss_dirty', 'referenced']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0267b5c2-9e49-472e-8e71-18e3266b02f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
