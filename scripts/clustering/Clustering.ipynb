{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fddbf330-399a-4939-b345-ea2a28446cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Used to accelerate plotting DAMON figures.\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "\n",
    "from matplotlib.colors import LogNorm, hsv_to_rgb\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def apply_cluster(page_stat_df):\n",
    "    scaler = StandardScaler()\n",
    "    features = page_stat_df.drop(columns=['PageFrame', 'rno', 'duty_cycle_sample_count', 'duty_cycle'])\n",
    "    #print(features)\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    \n",
    "    #pca_col = ['pc1', 'pc2']\n",
    "    pca = PCA(n_components=0.95)\n",
    "    pca_df = pd.DataFrame(pca.fit_transform(scaled_features))#, columns=pca_col)\n",
    "    \n",
    "    k = 4\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    #kmeans.fit(scaled_features)\n",
    "    #kmeans.fit(pca_df)\n",
    "    db = DBSCAN(eps=1.0, min_samples=5).fit(pca_df)\n",
    "    \n",
    "    #page_stat_df['cluster'] = kmeans.labels_\n",
    "    page_stat_df['cluster'] = db.labels_\n",
    "    page_stat_df['cluster'] = page_stat_df['cluster'].astype(int)\n",
    "\n",
    "    #print('----page_stat_df-----')\n",
    "    #print(page_stat_df)\n",
    "    #print('----------')\n",
    "    #print('pca_df')\n",
    "    #print(pca_df)\n",
    "    #print('-----')\n",
    "    \n",
    "    page_stat_df_merged = (pd.concat([page_stat_df, pca_df], axis=1))\n",
    "    page_stat_df_merged['start_epoch'] = 0\n",
    "\n",
    "    # Return new df with cluster labels and pca values\n",
    "    return page_stat_df_merged\n",
    "\n",
    "def find_region_id(row, df2):\n",
    "    #print(row)\n",
    "    #time = row['time']\n",
    "    addr = row['PageFrame']\n",
    "    matches = df2[\n",
    "            (df2['start'] <= addr) &\n",
    "            (df2['end'] > addr)\n",
    "            #(df2['start_addr'] <= addr) &\n",
    "            #(df2['end_addr'] >= addr)\n",
    "            ]\n",
    "    if not matches.empty:\n",
    "        return matches.iloc[0]['rno'].astype(int) # if multiple matches, take the first\n",
    "    else:\n",
    "        #print(\"Failed! time {} addr {}\".format(time,addr))\n",
    "        #exit()\n",
    "        return None\n",
    "\n",
    "# Prepare a df for given PEBS sample file\n",
    "def prepare_pebs_df(file):\n",
    "    # Read the file line by line\n",
    "    with open(file) as f:\n",
    "        rows = [line.strip().split() for line in f if line.strip()]\n",
    "\n",
    "    # Find the maximum number of columns in any row\n",
    "    max_cols = max(len(row) for row in rows)\n",
    "\n",
    "    # Pad each row so all have the same length\n",
    "    #padded_rows = [row + [np.nan]*(max_cols - len(row)) for row in rows]\n",
    "\n",
    "    # Function to pad each row with the last recorded value\n",
    "    def pad_row(row, target_length):\n",
    "        if len(row) < target_length:\n",
    "            last_value = row[-1]\n",
    "            # Extend the row with the last_value until it reaches the target length\n",
    "            row = row + [last_value] * (target_length - len(row))\n",
    "        return row\n",
    "\n",
    "    # Pad each row accordingly\n",
    "    padded_rows = [pad_row(row, max_cols) for row in rows]\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(padded_rows)\n",
    "\n",
    "    # Rename columns: first column as 'PageFrame' and remaining as 'Epoch1', 'Epoch2', ...\n",
    "    df.rename(columns={0: \"PageFrame\"}, inplace=True)\n",
    "    df.columns = [\"PageFrame\"] + [f\"Epoch_{i}\" for i in range(1, max_cols)]\n",
    "\n",
    "    df[\"PageFrame\"] = df[\"PageFrame\"].apply(lambda x: hex(int(x, 16) << 21))\n",
    "\n",
    "    # Convert epoch columns to numeric\n",
    "    for col in df.columns[1:]:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "\n",
    "    # Set PageFrame as index for easier time-series operations\n",
    "    df.set_index(\"PageFrame\", inplace=True)\n",
    "\n",
    "    df = df.copy() # Improves performance? df is sparse otherwise\n",
    "\n",
    "    # Compute the deltas across epochs\n",
    "    delta_df = df.diff(axis=1)\n",
    "\n",
    "    # For the first epoch, fill NaN with the original epoch value\n",
    "    first_epoch = df.columns[0]\n",
    "    delta_df[first_epoch] = df[first_epoch]\n",
    "\n",
    "    # Reorder columns to ensure the first epoch is first\n",
    "    delta_df = delta_df[df.columns]\n",
    "\n",
    "    # Optional: Convert column names to a numeric index if desired\n",
    "    # For plotting purposes, we can remove the 'Epoch_' prefix and convert to int\n",
    "    delta_df.columns = [int(col.replace(\"Epoch_\", \"\"))*0.5 for col in delta_df.columns]\n",
    "\n",
    "    # If we want to use plt instead of sns, melt df into long form\n",
    "    df_long = (\n",
    "        delta_df\n",
    "        .reset_index()\n",
    "        .melt(id_vars=[\"PageFrame\"], var_name=\"epoch\", value_name=\"value\")\n",
    "    )\n",
    "    df_long[\"PageFrame\"] = df_long[\"PageFrame\"].apply(lambda x: int(x,16))\n",
    "    return df_long\n",
    "\n",
    "    return delta_df\n",
    "\n",
    "def get_reuse_distance_df(df):\n",
    "    df_zero_streak_sorted = df.sort_values(by=['PageFrame', 'epoch']).reset_index(drop=True)\n",
    "    \n",
    "    # Container for results\n",
    "    results = []\n",
    "    \n",
    "    # Group by PageFrame\n",
    "    for pf, group in df_zero_streak_sorted.groupby('PageFrame'):\n",
    "        # Mark where value == 0\n",
    "        zero_mask = group['value'] == 0\n",
    "    \n",
    "        # Identify start of new streaks using the change in zero_mask\n",
    "        streak_id = (zero_mask != zero_mask.shift()).cumsum()\n",
    "    \n",
    "        # For value == 0 streaks only, compute their lengths\n",
    "        zero_streaks = group[zero_mask].groupby(streak_id).size()\n",
    "    \n",
    "        # Get the max streak length (0 if none)\n",
    "        max_streak = zero_streaks.max() if not zero_streaks.empty else 0\n",
    "    \n",
    "        results.append({'PageFrame': pf, 'reuse_distance': max_streak})\n",
    "    \n",
    "    # Create a new dataframe\n",
    "    streak_df = pd.DataFrame(results)\n",
    "    return streak_df\n",
    "\n",
    "def calculate_duty_cycle(df):\n",
    "    # Calculate Duty Cycle\n",
    "    non_zero_df = df[df['value'] != 0]\n",
    "    counts = non_zero_df.groupby('PageFrame').size()\n",
    "    counts.name = 'duty_cycle'\n",
    "    df = df.merge(counts, on='PageFrame', how='left')\n",
    "    df['duty_cycle'] = df['duty_cycle'].fillna(0).astype(int)\n",
    "    df['duty_cycle_sample_count'] = len(df['epoch'].unique())\n",
    "    df['duty_cycle_percent'] = (df['duty_cycle'] / len(df['epoch'].unique())*100).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb81a9f5-9688-4c26-9b6d-29ee9c300805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch  rno            start              end     inode  \\\n",
      "0       0    0   93824992231424   93824992235520  92030597   \n",
      "1       0    1   93824992235520   93824992256000  92030597   \n",
      "2       0    2   93824992256000   93824992264192  92030597   \n",
      "3       0    3   93824992264192   93824992268288  92030597   \n",
      "4       0    4   93824992268288   93824992272384  92030597   \n",
      "5       0    5   93824992272384   93824992407552         0   \n",
      "6      57    6  140669173792768  140669182181376         0   \n",
      "7      57    7  140669182185472  140669190574080         0   \n",
      "8      57    8  140669190578176  140669198966784         0   \n",
      "9      57    9  140669198970880  140669207359488         0   \n",
      "10     57   10  140669207363584  140669215752192         0   \n",
      "11     57   11  140669215756288  140669224144896         0   \n",
      "12     57   12  140669224148992  140737349943296         0   \n",
      "13      0    7  140737349943296  140737350107136    246393   \n",
      "14      0    8  140737350107136  140737351766016    246393   \n",
      "15      0    9  140737351766016  140737352126464    246393   \n",
      "16      0   10  140737352130560  140737352146944    246393   \n",
      "17      0   11  140737352146944  140737352155136    246393   \n",
      "18      0   12  140737352155136  140737352208384         0   \n",
      "19      0   13  140737353510912  140737353523200         0   \n",
      "20      0   14  140737353523200  140737353564160    246606   \n",
      "21      0   15  140737353564160  140737353764864    246606   \n",
      "22      0   16  140737353764864  140737353818112    246606   \n",
      "23      0   17  140737353818112  140737353822208    246606   \n",
      "24      0   18  140737353822208  140737353826304    246606   \n",
      "25      0   19  140737353854976  140737353863168         0   \n",
      "26      0   20  140737353879552  140737353887744         0   \n",
      "27      0   21  140737353887744  140737353895936    246279   \n",
      "28      0   22  140737353895936  140737354067968    246279   \n",
      "29      0   23  140737354067968  140737354113024    246279   \n",
      "30      0   24  140737354117120  140737354125312    246279   \n",
      "31      0   25  140737354125312  140737354133504    246279   \n",
      "32      0   26  140737488216064  140737488351232         0   \n",
      "\n",
      "                                             pathname      size    rss_kb  \\\n",
      "0   /mydata/workloads/XSBench/openmp-threading/XSB...         4         4   \n",
      "1   /mydata/workloads/XSBench/openmp-threading/XSB...        20        20   \n",
      "2   /mydata/workloads/XSBench/openmp-threading/XSB...         8         8   \n",
      "3   /mydata/workloads/XSBench/openmp-threading/XSB...         4         4   \n",
      "4   /mydata/workloads/XSBench/openmp-threading/XSB...         4         4   \n",
      "5                                              [heap]       132         8   \n",
      "6                                                 NaN      8192         8   \n",
      "7                                                 NaN      8192         8   \n",
      "8                                                 NaN      8192         8   \n",
      "9                                                 NaN      8192         8   \n",
      "10                                                NaN      8192         8   \n",
      "11                                                NaN      8192         8   \n",
      "12                                                NaN  66529096  66520912   \n",
      "13                /usr/lib/x86_64-linux-gnu/libc.so.6       160       160   \n",
      "14                /usr/lib/x86_64-linux-gnu/libc.so.6      1620      1088   \n",
      "15                /usr/lib/x86_64-linux-gnu/libc.so.6       352       168   \n",
      "16                /usr/lib/x86_64-linux-gnu/libc.so.6        16        16   \n",
      "17                /usr/lib/x86_64-linux-gnu/libc.so.6         8         8   \n",
      "18                                                NaN        52        20   \n",
      "19                                                NaN        12         8   \n",
      "20         /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0        40        40   \n",
      "21         /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0       196       192   \n",
      "22         /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0        52        48   \n",
      "23         /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0         4         4   \n",
      "24         /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0         4         4   \n",
      "25                                                NaN         8         8   \n",
      "26                                             [vdso]         8         4   \n",
      "27     /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2         8         8   \n",
      "28     /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2       168       168   \n",
      "29     /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2        44        40   \n",
      "30     /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2         8         8   \n",
      "31     /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2         8         8   \n",
      "32                                            [stack]       132        12   \n",
      "\n",
      "      pss_kb  pss_dirty  referenced  \n",
      "0          4          0           4  \n",
      "1         20          0          20  \n",
      "2          8          0           8  \n",
      "3          4          4           4  \n",
      "4          4          4           4  \n",
      "5          8          8           8  \n",
      "6          8          8           8  \n",
      "7          8          8           8  \n",
      "8          8          8           8  \n",
      "9          8          8           8  \n",
      "10         8          8           8  \n",
      "11         8          8           8  \n",
      "12  66520912   66520912    66520912  \n",
      "13         4          0         160  \n",
      "14        25          0        1088  \n",
      "15         4          0         168  \n",
      "16        16         16          16  \n",
      "17         8          8           8  \n",
      "18        20         20          20  \n",
      "19         8          8           8  \n",
      "20        40          0          40  \n",
      "21       192          0         192  \n",
      "22        48          0          48  \n",
      "23         4          4           4  \n",
      "24         4          4           4  \n",
      "25         8          8           8  \n",
      "26         0          0           4  \n",
      "27         0          0           8  \n",
      "28         4          0         168  \n",
      "29         1          0          40  \n",
      "30         8          8           8  \n",
      "31         8          8           8  \n",
      "32        12         12          12  \n"
     ]
    }
   ],
   "source": [
    "vma_df = (pd.read_csv('../../results/results_vma_cluster/XSBench_memory_regions_smap_deduplicated.csv'))\n",
    "\n",
    "next_rno = vma_df['rno'].max() + 1\n",
    "\n",
    "vma_df['start'] = vma_df['start'].apply(lambda x: int(x,16))\n",
    "vma_df['end'] = vma_df['end'].apply(lambda x: int(x,16))\n",
    "print(vma_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb1ddf5c-292f-4e66-9f93-b703e32a871c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch  rno            start              end  inode pathname      size  \\\n",
      "12     57   12  140669224148992  140737349943296      0      NaN  66529096   \n",
      "\n",
      "      rss_kb    pss_kb  pss_dirty  referenced  \n",
      "12  66520912  66520912   66520912    66520912  \n",
      "    epoch  rno            start              end  inode  pathname     size  \\\n",
      "12     57   27  140669224148992  140670297890816      0       NaN  1048576   \n",
      "12     57   28  140670297890816  140671371632640      0       NaN  1048576   \n",
      "12     57   29  140671371632640  140672445374464      0       NaN  1048576   \n",
      "12     57   30  140672445374464  140673519116288      0       NaN  1048576   \n",
      "12     57   31  140673519116288  140674592858112      0       NaN  1048576   \n",
      "..    ...  ...              ...              ...    ...       ...      ...   \n",
      "12     57   86  140732574916608  140733648658432      0       NaN  1048576   \n",
      "12     57   87  140733648658432  140734722400256      0       NaN  1048576   \n",
      "12     57   88  140734722400256  140735796142080      0       NaN  1048576   \n",
      "12     57   89  140735796142080  140736869883904      0       NaN  1048576   \n",
      "12     57   90  140736869883904  140737349943296      0       NaN   468808   \n",
      "\n",
      "      rss_kb    pss_kb  pss_dirty  referenced  \n",
      "12  66520912  66520912   66520912    66520912  \n",
      "12  66520912  66520912   66520912    66520912  \n",
      "12  66520912  66520912   66520912    66520912  \n",
      "12  66520912  66520912   66520912    66520912  \n",
      "12  66520912  66520912   66520912    66520912  \n",
      "..       ...       ...        ...         ...  \n",
      "12  66520912  66520912   66520912    66520912  \n",
      "12  66520912  66520912   66520912    66520912  \n",
      "12  66520912  66520912   66520912    66520912  \n",
      "12  66520912  66520912   66520912    66520912  \n",
      "12  66520912  66520912   66520912    66520912  \n",
      "\n",
      "[64 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get only vma with no pathname (anon region) and a size over 2 MB\n",
    "filtered_vma_df = (vma_df[pd.isna(vma_df['pathname']) & (vma_df['size'] >= (1<<21))])\n",
    "#filtered_vma_df = vma_df\n",
    "print(filtered_vma_df)\n",
    "def split_large_rows(df, next_rno, size_threshold=(1<<20)):\n",
    "    new_rows = []\n",
    "    #next_rno = df['rno'].max() + 1  # Start new rno from max + 1\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if row['size'] > size_threshold:\n",
    "            # Calculate number of chunks needed\n",
    "            num_chunks = int(row['size'] // size_threshold)\n",
    "            last_chunk_size = row['size'] % size_threshold\n",
    "\n",
    "            # Split into chunks\n",
    "            start = row['start']\n",
    "            for i in range(num_chunks):\n",
    "                new_row = row.copy()\n",
    "                new_row['rno'] = next_rno\n",
    "                new_row['start'] = start\n",
    "                new_row['end'] = start + size_threshold * (1<<10)\n",
    "                new_row['size'] = size_threshold\n",
    "                new_rows.append(new_row)\n",
    "                start += size_threshold * (1<<10)\n",
    "                next_rno += 1\n",
    "\n",
    "            # Last chunk (if any remainder)\n",
    "            if last_chunk_size > 0:\n",
    "                new_row = row.copy()\n",
    "                new_row['rno'] = next_rno\n",
    "                new_row['start'] = start\n",
    "                new_row['end'] = start + last_chunk_size * (1<<10)\n",
    "                new_row['size'] = last_chunk_size\n",
    "                new_rows.append(new_row)\n",
    "                next_rno += 1\n",
    "        else:\n",
    "            new_rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "split_vma_df = (split_large_rows(filtered_vma_df, next_rno))\n",
    "print(split_vma_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1770f8f2-498e-4c53-ba66-77190097b129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to parse string \"03ff8398\" at position 1159",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2391\u001b[39m, in \u001b[36mpandas._libs.lib.maybe_convert_numeric\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Unable to parse string \"03ff8398\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Read in pebs data and bin in N second intervals\u001b[39;00m\n\u001b[32m      2\u001b[39m N = \u001b[32m10\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = prepare_pebs_df(\u001b[33m'\u001b[39m\u001b[33m../../results/results_vma_cluster/xsbench_xsbench_samples.dat\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mtime_bin\u001b[39m\u001b[33m'\u001b[39m] = (df[\u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m] // N).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mprepare_pebs_df\u001b[39m\u001b[34m(file)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Convert epoch columns to numeric\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df.columns[\u001b[32m1\u001b[39m:]:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     df[col] = pd.to_numeric(df[col])\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# Set PageFrame as index for easier time-series operations\u001b[39;00m\n\u001b[32m    108\u001b[39m df.set_index(\u001b[33m\"\u001b[39m\u001b[33mPageFrame\u001b[39m\u001b[33m\"\u001b[39m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dataVis/lib/python3.13/site-packages/pandas/core/tools/numeric.py:232\u001b[39m, in \u001b[36mto_numeric\u001b[39m\u001b[34m(arg, errors, downcast, dtype_backend)\u001b[39m\n\u001b[32m    230\u001b[39m coerce_numeric = errors \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     values, new_mask = lib.maybe_convert_numeric(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m    233\u001b[39m         values,\n\u001b[32m    234\u001b[39m         \u001b[38;5;28mset\u001b[39m(),\n\u001b[32m    235\u001b[39m         coerce_numeric=coerce_numeric,\n\u001b[32m    236\u001b[39m         convert_to_masked_nullable=dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default\n\u001b[32m    237\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values_dtype, StringDtype)\n\u001b[32m    238\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m values_dtype.storage == \u001b[33m\"\u001b[39m\u001b[33mpyarrow_numpy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    239\u001b[39m     )\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2433\u001b[39m, in \u001b[36mpandas._libs.lib.maybe_convert_numeric\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Unable to parse string \"03ff8398\" at position 1159"
     ]
    }
   ],
   "source": [
    "# Read in pebs data and bin in N second intervals\n",
    "N = 10\n",
    "df = prepare_pebs_df('../../results/results_vma_cluster/xsbench_xsbench_samples.dat')\n",
    "df['time_bin'] = (df['epoch'] // N).astype(int)\n",
    "print(df)\n",
    "dfs_by_interval = {\n",
    "    f\"{N * bin}s_to_{N * (bin + 1)}s\": group.drop(columns='time_bin')\n",
    "    for bin, group in df.groupby('time_bin')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3dcfa6-31f5-4d5b-a08e-7f52f5ff4b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_stat_df = None\n",
    "labeled_dfs = []\n",
    "i = 0\n",
    "for df in dfs_by_interval.values():\n",
    "    #if df['epoch'].min() < 70:\n",
    "    #    continue\n",
    "    time_bin_df = df.copy()\n",
    "    duty_df = calculate_duty_cycle(time_bin_df)\n",
    "    duty_df = duty_df.drop_duplicates(subset='PageFrame')[['PageFrame', 'duty_cycle', 'duty_cycle_sample_count', 'duty_cycle_percent']]\n",
    "    #duty_avg_df = (df.drop(columns=['epoch', 'duty_cycle_sample_count', 'duty_cycle', 'value']).groupby('PageFrame', as_index=False).mean().astype(int))\n",
    "\n",
    "    #print(duty_df)\n",
    "    streak_df = get_reuse_distance_df(time_bin_df)\n",
    "    time_bin_df = time_bin_df.merge(streak_df, on='PageFrame', how='left')\n",
    "    \n",
    "    #print(streak_df)\n",
    "    #break\n",
    "    page_stat_df = time_bin_df.groupby('PageFrame').agg(\n",
    "        {\n",
    "            'value': ['mean', 'std', 'min', 'max'],\n",
    "            'reuse_distance': ['mean']\n",
    "        }\n",
    "    )\n",
    "    #print('time bin...---------')\n",
    "    #print(time_bin_df)\n",
    "    #print('page_stat ..........')\n",
    "    #print(page_stat_df)\n",
    "\n",
    "    # Combine duty cycle info with access statistics\n",
    "    page_stat_df.columns = ['_'.join(col) for col in page_stat_df.columns]\n",
    "    page_stat_df = page_stat_df.merge(duty_df, on='PageFrame', how='left')\n",
    "    page_stat_df = page_stat_df.reset_index(drop=True)\n",
    "\n",
    "    # Apply region numbers, do this last on smaller aggregated data set because it takes a while.\n",
    "    page_stat_df['rno'] = page_stat_df.apply(lambda row: find_region_id(row, split_vma_df), axis=1)\n",
    "    #page_stat_df['rno'] = page_stat_df['rno'].fillna(-1)\n",
    "    #print(page_stat_df)\n",
    "    page_stat_df = page_stat_df.dropna().reset_index(drop=True)\n",
    "    page_stat_df['rno'] = page_stat_df['rno'].astype(int)\n",
    "\n",
    "    #print(page_stat_df)\n",
    "    #print('-----')\n",
    "    clustered_df = apply_cluster(page_stat_df.copy())\n",
    "\n",
    "    time_bin_df = time_bin_df.merge(\n",
    "        clustered_df[['PageFrame', 'cluster']].drop_duplicates('PageFrame'),\n",
    "        on='PageFrame',\n",
    "        how='left'\n",
    "    )\n",
    "    time_bin_df = time_bin_df.dropna()\n",
    "    labeled_dfs.append(time_bin_df)\n",
    "    print(i)\n",
    "    #if i > 1:\n",
    "    #    break\n",
    "    i+=1\n",
    "\n",
    "    #if time_bin_df['epoch'].min() > 70:\n",
    "    #    break\n",
    "\n",
    "print(\"Labeled DFS==============\")\n",
    "#print(page_stat_df)\n",
    "print(labeled_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeaa19c-34b5-46a7-97c5-1c0872a2b6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874eea98-9174-441e-afab-b87ca51ab56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat(labeled_dfs, ignore_index=True)\n",
    "print(final_df)\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.scatter(final_df['epoch'], final_df['PageFrame'], c=final_df['cluster'], s=50, edgecolor='none', rasterized=True, alpha=0.7, marker='.')\n",
    "\n",
    "#df.plot.scatter(x='x', y='y', c='cluster', colormap='viridis', label='Clusters')\n",
    "#final_df.plot.scatter(x ='epoch', y='PageFrame', c='cluster',label='Clusters', s=50, edgecolor='none', rasterized=True, alpha=0.7, marker='.')\n",
    "\n",
    "#plt.legend()  # sometimes required explicitly\n",
    "#plt.show()\n",
    "\n",
    "#plt.scatter(df['time'], df['address'], s=50, edgecolor='none', rasterized=True, alpha=0.7, marker='.')\n",
    "xmin = final_df['epoch'].min()\n",
    "xmax = final_df['epoch'].max()\n",
    "ymin = final_df['PageFrame'].min() + (1<<30)\n",
    "#plt.hlines(y=ymin, xmin=xmin, xmax=xmax, colors='red', linestyles='dashed')\n",
    "ax = plt.gca()\n",
    "\n",
    "# 1) Define a hex‐formatter: takes a float x and returns e.g. '0x1a3f'\n",
    "hex_formatter = FuncFormatter(lambda x, pos: hex(int(x)))\n",
    "\n",
    "# 2) Install it on the y‐axis\n",
    "ax.yaxis.set_major_formatter(hex_formatter)\n",
    "ax.invert_yaxis()\n",
    "#sns.scatterplot(data=time_bin_df1.dropna(), y='PageFrame', x='epoch', hue='cluster', palette=sns.color_palette(\"tab10\"))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa51e32d-4c78-42db-8b27-0b40ba97cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labeled_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd9087-e2a2-4519-8790-22bc35222e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_bin_df['cluster'] = clustered\n",
    "#time_bin_df['cluster'] = time_bin_df['PageFrame'].map(clustered_df.set_index('PageFrame')['cluster'])\n",
    "time_bin_df1 = time_bin_df.merge(\n",
    "    clustered_df[['PageFrame', 'cluster']].drop_duplicates('PageFrame'),\n",
    "    on='PageFrame',\n",
    "    how='left'\n",
    ")\n",
    "#print(clustered_df)\n",
    "#time_bin_df1['cluster'] = (time_bin_df1['cluster'].fillna(-1))\n",
    "time_bin_df1 = (time_bin_df1.dropna())\n",
    "print(time_bin_df1)\n",
    "#plt.scatter(time_bin_df1['epoch'], time_bin_df1['PageFrame'], c=time_bin_df1['cluster'])\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.scatter(time_bin_df1['epoch'], time_bin_df1['PageFrame'], c=time_bin_df1['cluster'], s=50, edgecolor='none', rasterized=True, alpha=0.7, marker='.')\n",
    "#plt.scatter(df['time'], df['address'], s=50, edgecolor='none', rasterized=True, alpha=0.7, marker='.')\n",
    "xmin = df['epoch'].min()\n",
    "xmax = df['epoch'].max()\n",
    "ymin = time_bin_df1['PageFrame'].min() + (1<<30)\n",
    "plt.hlines(y=ymin, xmin=xmin, xmax=xmax, colors='red', linestyles='dashed')\n",
    "ax = plt.gca()\n",
    "\n",
    "# 1) Define a hex‐formatter: takes a float x and returns e.g. '0x1a3f'\n",
    "hex_formatter = FuncFormatter(lambda x, pos: hex(int(x)))\n",
    "\n",
    "# 2) Install it on the y‐axis\n",
    "ax.yaxis.set_major_formatter(hex_formatter)\n",
    "ax.invert_yaxis()\n",
    "#sns.scatterplot(data=time_bin_df1.dropna(), y='PageFrame', x='epoch', hue='cluster', palette=sns.color_palette(\"tab10\"))\n",
    "plt.show()\n",
    "#page_stat_df['rno'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dffaf56-f994-4ba6-8cc4-f65dea78f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clustered_df)\n",
    "\n",
    "sns.scatterplot(data=clustered_df, y='pc2', x='pc1', hue='cluster', palette=sns.color_palette(\"tab10\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176511b4-aed9-4007-b7ce-dbda5e61a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_pebs_df(file):\n",
    "    # Read the file line by line\n",
    "    with open(file) as f:\n",
    "        rows = [line.strip().split() for line in f if line.strip()]\n",
    "\n",
    "    # Find the maximum number of columns in any row\n",
    "    max_cols = max(len(row) for row in rows)\n",
    "\n",
    "    # Pad each row so all have the same length\n",
    "    #padded_rows = [row + [np.nan]*(max_cols - len(row)) for row in rows]\n",
    "\n",
    "    # Function to pad each row with the last recorded value\n",
    "    def pad_row(row, target_length):\n",
    "        if len(row) < target_length:\n",
    "            last_value = row[-1]\n",
    "            # Extend the row with the last_value until it reaches the target length\n",
    "            row = row + [last_value] * (target_length - len(row))\n",
    "        return row\n",
    "\n",
    "    # Pad each row accordingly\n",
    "    padded_rows = [pad_row(row, max_cols) for row in rows]\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(padded_rows)\n",
    "\n",
    "    # Rename columns: first column as 'PageFrame' and remaining as 'Epoch1', 'Epoch2', ...\n",
    "    df.rename(columns={0: \"PageFrame\"}, inplace=True)\n",
    "    df.columns = [\"PageFrame\"] + [f\"Epoch_{i}\" for i in range(1, max_cols)]\n",
    "\n",
    "    df[\"PageFrame\"] = df[\"PageFrame\"].apply(lambda x: hex(int(x, 16) << 21))\n",
    "\n",
    "    # Convert epoch columns to numeric\n",
    "    for col in df.columns[1:]:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "\n",
    "    # Set PageFrame as index for easier time-series operations\n",
    "    df.set_index(\"PageFrame\", inplace=True)\n",
    "\n",
    "    df = df.copy() # Improves performance? df is sparse otherwise\n",
    "\n",
    "    # Compute the deltas across epochs\n",
    "    delta_df = df.diff(axis=1)\n",
    "\n",
    "    # For the first epoch, fill NaN with the original epoch value\n",
    "    first_epoch = df.columns[0]\n",
    "    delta_df[first_epoch] = df[first_epoch]\n",
    "\n",
    "    # Reorder columns to ensure the first epoch is first\n",
    "    delta_df = delta_df[df.columns]\n",
    "\n",
    "    # Optional: Convert column names to a numeric index if desired\n",
    "    # For plotting purposes, we can remove the 'Epoch_' prefix and convert to int\n",
    "    delta_df.columns = [int(col.replace(\"Epoch_\", \"\"))*0.5 for col in delta_df.columns]\n",
    "\n",
    "    # If we want to use plt instead of sns, melt df into long form\n",
    "    #df_long = (\n",
    "    #    delta_df\n",
    "    #    .reset_index()\n",
    "    #    .melt(id_vars=[\"PageFrame\"], var_name=\"epoch\", value_name=\"value\")\n",
    "    #)\n",
    "    #df_long[\"PageFrame\"] = df_long[\"PageFrame\"].apply(lambda x: int(x,16))\n",
    "    #return df_long\n",
    "\n",
    "    return delta_df\n",
    "\n",
    "def generate_pebs_figure(file, output_path):\n",
    "\n",
    "    output_file = output_path + \"_heatmap.png\"\n",
    "    print(\"Checking {}\".format(output_file))\n",
    "\n",
    "    if os.path.isfile(output_file):\n",
    "        print(\"Skipping {}\".format(output_file))\n",
    "        return\n",
    "\n",
    "    df = prepare_pebs_df(file)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    #sns.heatmap(df, cmap=\"viridis\", cbar=True, norm=LogNorm())\n",
    "\n",
    "    xmin = df['epoch'].min()\n",
    "    xmax = df['epoch'].max()\n",
    "    \n",
    "    # Draw a horizontal line at y = some_value\n",
    "    ymax = time_bin_df1['PageFrame'].max()\n",
    "    ymin = time_bin_df1['PageFrame'].min()\n",
    "    plt.hlines(y=ymax, xmin=xmin, xmax=xmax, colors='red', linestyles='dashed')\n",
    "    plt.hlines(y=ymin, xmin=xmin, xmax=xmax, colors='red', linestyles='dashed')\n",
    "\n",
    "    df = df[df['PageFrame'] >= ymin]\n",
    "    df = df[df['PageFrame'] <= ymax]\n",
    "    # If we want to use plt instead of sns\n",
    "    plt.scatter(df['epoch'], df['PageFrame'], c=df['value'], s=50, norm=LogNorm(), edgecolor='none', rasterized=True, alpha=0.7, marker='.')\n",
    "\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ## 1) Define a hex‐formatter: takes a float x and returns e.g. '0x1a3f'\n",
    "    hex_formatter = FuncFormatter(lambda x, pos: hex(int(x)))\n",
    "\n",
    "    ## 2) Install it on the y‐axis\n",
    "    ax.yaxis.set_major_formatter(hex_formatter)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Page Frame\")\n",
    "    plt.title(file + \": PEBS\")\n",
    "    plt.show()\n",
    "    #plt.savefig(output_file, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "generate_pebs_figure('../../results/results_vma_cluster/xsbench_xsbench_samples.dat', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d9c4c-186c-4869-9fc8-abec4079a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#vma_stat_df = page_stat_df.groupby('rno').agg(\n",
    "#        {\n",
    "##            'duty_cycle_percent': ['count', 'mean', 'std', 'min', 'max'],\n",
    "#            'reuse_distance': ['count', 'mean', 'std', 'min', 'max']\n",
    "#        }\n",
    "#    )\n",
    "\n",
    "#filtered_vma_df = vma_df\n",
    "#filtered_vma_df = vma_df[vma_df['pathname'] == pd.NA]\n",
    "\n",
    "#vma_stat_df.columns = ['_'.join(col) for col in vma_stat_df.columns]\n",
    "#vma_stat_df = vma_stat_df.reset_index()\n",
    "#vma_stat_df = pd.merge(vma_stat_df, filtered_vma_df, on='rno', how='left')\n",
    "#vma_stat_df['rss_kb_percent'] = vma_stat_df['rss_kb'] / vma_stat_df['size']\n",
    "#vma_stat_df['pss_kb_dirty_percent'] = vma_stat_df['pss_dirty'] / vma_stat_df['pss_kb']\n",
    "#print(vma_stat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51e6d8-d77e-40f7-ab52-8fcfb031f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out VMAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3a403-44a2-4d08-a350-62b76d046603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vma_stat_df = vma_stat_df.loc[vma_stat_df.groupby('rno')['size'].idxmax()]\n",
    "#print(vma_stat_df.reset_index(drop=True).drop(columns=['epoch', 'start', 'end', 'inode', 'pathname', 'rss_kb', 'pss_kb', 'pss_dirty', 'referenced']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0267b5c2-9e49-472e-8e71-18e3266b02f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
